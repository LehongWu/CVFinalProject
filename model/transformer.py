import torch
import torch.nn as nn
from torch import Tensor
from typing import List, Callable, Union, Any, TypeVar
import torch.nn.functional as F
from abc import abstractmethod
from .drop import DropPath
# from .VQVAE4transformer_ver3 import VQVAE # use this when tiny image net
from .VQVAE4transformer import VQVAE
import math
import warnings
import random

class InputGenerator(nn.Module):
    def __init__(self,
                 vae_path: str, 
                 in_channels: int,
                 embedding_dim: int,
                 num_embeddings: int,
                 hidden_dims: List = None,
                 encoder_depth: int = 6,
                 decoder_depth: int = 6,
                 beta: float = 0.25,
                 img_size: int = 32,
                 activation_layer: nn.Module = nn.SiLU,
                 norm_layer: nn.Module = nn.LayerNorm,
                 **kwargs) -> None:
        super(InputGenerator, self).__init__()

        self.vqvae = VQVAE(in_channels=in_channels,
                           embedding_dim=embedding_dim, 
                           num_embeddings=num_embeddings,
                           hidden_dims=hidden_dims, 
                           encoder_depth=encoder_depth, 
                           decoder_depth=decoder_depth, 
                           beta=beta, 
                           img_size=img_size, 
                           activation_layer=activation_layer, 
                           norm_layer=norm_layer, 
                           kwargs=kwargs)

        self.vqvae.requires_grad_(False)
        # print("load vqvae from " + vae_path + " to initialize input_generator.")
        # self.vqvae.load_state_dict(torch.load(vae_path), strict=False)

    def reconstruct(self, input):

        encoding = self.vqvae.encode(input)[0]

        quantized_inputs, vq_loss = self.vqvae.vq_layer(encoding)

        recons = self.vqvae.decode(quantized_inputs)
        
        return recons
    

    def generate_tokens(self, input):
        """
        Using pretrained VQ-VAE to map input to tokens
        :param input: (Tensor) Input tensor to encoder [N x C x H x W]
        :return: (Tensor) latent codes [N, D, H', W']
        """
        result = self.vqvae.encode(input)[0]
        quantized_result, _ = self.vqvae.vq_layer(result)
        # N, D, H, W = result.shape
        # result = result.permute(0, 2, 3, 1).contiguous().view(N, H * W, D)
        return quantized_result
        

    def generate_mask(self, N, L, mask_ratio):
        """
        generate mask for input tokens
        :param input: 
        N: int, batch size
        L: int, number of tokens per image, H' * W'
        mask_ratio: float, only work when mode is constant 
        :return: 
        mask: [N, L], True for masked
        """
        mask = torch.zeros((N, L), dtype=bool)
        mask[:, :] = False
        len_mask = int(L * mask_ratio)

        noise = torch.rand((N, L))
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is masked
        ids_mask = ids_shuffle[:, :len_mask]

        for i in range(N):
            mask[i, ids_mask[i, :]] = True
        
        return mask
    

    def generate_gt(self, tokens, mask):
        """
        generate ground truth for input tokens
        :param input: 
        tokens: from generate_tokens function
        mask: from generate_mask function
        :return: 
        gt: ground truth(-1 for NOT masked tokens) for input tokens
        """
        gt = self.vqvae.vq_layer.forward_index(tokens)
        gt[~mask] = -1
        return gt
    

    def generate_all(self, input, mask_ratio):
        """
        combine the three functions
        """
        tokens = self.generate_tokens(input)
        N, D, H, W = tokens.shape

        mask = self.generate_mask(N, H * W, mask_ratio)

        gt = self.generate_gt(tokens, mask)

        tokens = tokens.contiguous().view(N, D, -1).permute(0, 2, 1)

        return tokens, mask, gt

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = head_dim ** -0.5

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x, seqlen=1):
        B, N, C = x.shape
        
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)
        x = self.forward_attention(q, k, v)

        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def forward_attention(self, q, k, v):
        B, _, N, C = q.shape
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v
        x = x.transpose(1,2).reshape(B, N, C*self.num_heads)
        return x

class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., mlp_out_ratio=1.,
                 qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        # assert 'stage' in st_mode
        self.norm1 = norm_layer(dim)
        self.attn = Attention(dim, num_heads=num_heads,
                              qkv_bias=qkv_bias, qk_scale=qk_scale,
                              attn_drop=attn_drop, proj_drop=drop)
        
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        mlp_out_dim = int(dim * mlp_out_ratio)
        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim,
                       out_features=mlp_out_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, seqlen=1):
        x = x + self.drop_path(self.attn(self.norm1(x), seqlen))
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

class BidirectionalTransformer(nn.Module):
    def __init__(self,
                 num_patches: int, # 输入的序列长度（包括mask的）
                 num_embeds: int, # codebook长度
                 embed_dim: int, # 特征维数
                 depth: int, # attention层数
                 mlp_ratio: float, # MLP层隐藏层比输入维数的比率
                 num_heads: int, # attention头数
                 norm_layer: nn.Module, # 归一化方式，默认layer norm
                 num_class=10, # 类别数
                 ):
        super().__init__()
        # positional embedding
        # transfomer layer
        # read learned codebook 
        # ......
        self.num_patches = num_patches
        self.num_embeds = num_embeds
        self.embed_dim = embed_dim

        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        self.mask_token = nn.Parameter(torch.zeros(embed_dim))
        self.mix_weight = nn.Parameter(torch.ones(3)) # 特征融合的权重

        self.embed_layer = nn.Linear(embed_dim, embed_dim * 2)

        self.blocks = nn.ModuleList([
            Block(embed_dim * 2, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim * 2)

        self.pred_head = nn.Linear(embed_dim * 2, num_embeds)
        self.classify_head = nn.Linear(num_patches * embed_dim * 2, num_class)

        # 非mask的部分不计入loss，ground truth需置-1
        self.criterion = nn.CrossEntropyLoss(ignore_index=-1) 
        self.class_criterion = nn.CrossEntropyLoss()
        
        
    def initialize_weights(self):
        torch.nn.init.normal_(self.pos_embed, std=.02)

        # initialize nn.Linear and nn.LayerNorm
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def process_mask(self, x, mask):
        """
        input:
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        output:
        x - embedded features(w/ mask tokens): [N, H' * W', D]
        """
        # print(type(x[0, 0, 0].item()), type(mask[0, 0].item()), type(self.mask_token[0].item()))
        x[mask] = self.mask_token

        return x

    def forward_index(self, x, mask):
        """
        input: 
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        output: 
        indexes - predicted indexes for all tokens: [N, H' * W', num_embeds]
        """
        x = self.process_mask(x, mask)
        x = x + self.pos_embed
        x = self.embed_layer(x)

        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)

        indexes = self.pred_head(x)

        return indexes
    
    def forward_index_mixfeat(self, x, mask): # 特征融合版本
        """
        input: 
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        output: 
        indexes - predicted indexes for all tokens: [N, H' * W', num_embeds]
        """
        x = self.process_mask(x, mask)
        x = x + self.pos_embed
        x = self.embed_layer(x)
        feat = torch.zeros_like(x, device=x.device)
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if i % 2 == 1 and i != 7:
                feat += x * self.mix_weight[i // 2]
        feat = (feat + x) / (self.mix_weight.sum() + 1.)
        x = self.norm(feat)

        indexes = self.pred_head(x)

        return indexes
    
    def forward_class(self, x):
        with torch.no_grad():
            x = x + self.pos_embed
            x = self.embed_layer(x)

            for blk in self.blocks:
                x = blk(x)
            x = self.norm(x)

        feat = x.contiguous().view(x.shape[0], -1)
        classes = self.classify_head(feat)

        return classes
    
    def forward_loss(self, x, gt, mask):
        """
        input:
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        gt - ground truth indexes for tokens(-1 for no masked tokens): [N, H' * W']
        output:
        loss - cross entropy loss between predicted indexes and ground truth(only masked ones) 
        """
        N, L, D = x.shape
        # indexes = self.forward_index(x, mask)
        indexes = self.forward_index(x, mask)
        pred_indexes = torch.argmax(indexes, dim=2) # [N, L]
        indexes = indexes.contiguous().view(N * L, self.num_embeds)
        gt = gt.contiguous().view(N * L)
        loss = self.criterion(indexes, gt).mean()
        return loss, pred_indexes
        
    def forward_class_loss(self, x, labels):
        N, L, D = x.shape
        classes = self.forward_class(x)
        loss = self.class_criterion(classes, labels)
        return loss, classes


    def infer_one_iter(self, x, mask, t, T):
        """
        For test.
        Input: 
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        """
        N, L, D = x.shape
        n_pred = L - int(L * math.cos((math.pi / 2) * (t * 1. / T))) # 每轮保留的token数（包括之前轮预测的）
        # n_pred = int(L * t / T)
        n_keep = max(1, int(self.num_embeds * math.exp(-2. + (3. * -(t / T))))) # （每个位置随机选取index的宽容度，递减）
        with torch.no_grad():
            indexes = self.forward_index(x, mask)
            indexes = torch.softmax(indexes, dim=2)
        new_index_map = torch.zeros_like(mask, dtype=int)
        new_index_map[:, :] = -1
        new_mask = mask
        for n in range(N):
            select_index_list = torch.zeros(L, dtype=int)
            index_score_list = torch.zeros(L, dtype=float) # 选择的token的置信度
            for patch in range(L):
                if mask[n, patch]: # masked tokens
                    index_keep = torch.argsort(indexes[n, patch], descending=True)[:n_keep] # 降序排序，保留概率高的几个
                    id = random.randint(0, n_keep - 1)
                    select_index = index_keep[id] # 随机选取一个置信度较高的token
                    select_index_list[patch] = select_index
                    index_score_list[patch] = indexes[n, patch, select_index]
                else:
                    select_index_list[patch] = -1
                    index_score_list[patch] = 1.

            noise = torch.rand_like(index_score_list, device=index_score_list.device)
            patch_sorted = torch.argsort(index_score_list + noise, descending=True) # 按置信度降序排序
            patch_keep = patch_sorted[:n_pred] # 保留置信度高的patch的预测结果
            for patch in patch_keep:
                if mask[n, patch]:
                    index = select_index_list[patch]
                    new_index_map[n, patch] = index
                    # print(indexes[n, patch, index])
                    new_mask[n, patch] = False # 该位置产生预测结果，不再mask
        
        return new_index_map, new_mask

class ConditionBidirectionalTransformer(nn.Module): # 带label作为条件输入的
    def __init__(self, 
                 num_patches: int, # 输入的序列长度（包括mask的）
                 num_embeds: int, # codebook长度
                 embed_dim: int, # 特征维数
                 num_class: int, # label类数
                 depth: int, # attention层数
                 mlp_ratio: float, # MLP层隐藏层比输入维数的比率
                 num_heads: int, # attention头数
                 norm_layer: nn.Module, # 归一化方式，默认layer norm
                 ):
        super().__init__()
        # positional embedding
        # transfomer layer
        # read learned codebook 
        # ......
        self.num_patches = num_patches
        self.num_embeds = num_embeds
        self.embed_dim = embed_dim

        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
        self.mask_token = nn.Parameter(torch.zeros(embed_dim))
        self.cond_embed = nn.Parameter(torch.zeros(num_class, embed_dim))

        self.embed_layer = nn.Linear(embed_dim, embed_dim * 2)

        self.blocks = nn.ModuleList([
            Block(embed_dim * 2, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
            for i in range(depth)])
        self.norm = norm_layer(embed_dim * 2)

        self.pred_head = nn.Linear(embed_dim * 2, num_embeds)

        # 非mask的部分不计入loss，ground truth需置-1
        self.criterion = nn.CrossEntropyLoss(ignore_index=-1) 
        
        
    def initialize_weights(self):
        torch.nn.init.normal_(self.pos_embed, std=.02)
        torch.nn.init.normal_(self.mask_token, std=.02)
        torch.nn.init.normal_(self.cond_embed, std=.02)

        # initialize nn.Linear and nn.LayerNorm
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def process_mask(self, x, mask):
        """
        input:
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        output:
        x - embedded features(w/ mask tokens): [N, H' * W', D]
        """
        # print(type(x[0, 0, 0].item()), type(mask[0, 0].item()), type(self.mask_token[0].item()))
        x[mask] = self.mask_token

        return x

    def forward_index(self, x, mask, label):
        """
        input: 
        x - embedded features(w/ mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        output: 
        indexes - predicted indexes for all tokens: [N, H' * W', num_embeds]
        """
        x = self.process_mask(x, mask)
        x = x + self.pos_embed
        for n in range(x.shape[0]):
            x[n] += self.cond_embed[label[n].item()] 
        x = self.embed_layer(x)

        for blk in self.blocks:
            x = blk(x)
        x = self.norm(x)

        indexes = self.pred_head(x)

        return indexes
    
    def forward_loss(self, x, gt, mask, label):
        """
        input:
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        gt - ground truth indexes for tokens(-1 for no masked tokens): [N, H' * W']
        output:
        loss - cross entropy loss between predicted indexes and ground truth(only masked ones) 
        """
        N, L, D = x.shape
        indexes = self.forward_index(x, mask, label)
        pred_indexes = torch.argmax(indexes, dim=2) # [N, L]
        indexes = indexes.contiguous().view(N * L, self.num_embeds)
        gt = gt.contiguous().view(N * L)
        loss = self.criterion(indexes, gt).mean()
        return loss, pred_indexes
        
    def infer_one_iter(self, x, mask, t, T, label):
        """
        For test.
        Input: 
        x - embedded features(w/o mask tokens): [N, H' * W', D]
        mask - mask matrix, TRUE for mask and FALSE for no mask: [N, H' * W']
        """
        N, L, D = x.shape
        n_pred = L - int(L * math.cos((math.pi / 2) * (t * 1. / T))) # 每轮保留的token数（包括之前轮预测的）
        # n_pred = int(L * t / T)
        n_keep = max(1, int(self.num_embeds * math.exp(-2. + (3. * -(t / T))))) # （每个位置随机选取index的宽容度，递减）
        with torch.no_grad():
            indexes = self.forward_index(x, mask, label)
            indexes = torch.softmax(indexes, dim=2)
        new_index_map = torch.zeros_like(mask, dtype=int)
        new_index_map[:, :] = -1
        new_mask = mask
        for n in range(N):
            select_index_list = torch.zeros(L, dtype=int)
            index_score_list = torch.zeros(L, dtype=float) # 选择的token的置信度
            for patch in range(L):
                if mask[n, patch]: # masked tokens
                    index_keep = torch.argsort(indexes[n, patch], descending=True)[:n_keep] # 降序排序，保留概率高的几个
                    id = random.randint(0, n_keep - 1)
                    select_index = index_keep[id] # 随机选取一个置信度较高的token
                    select_index_list[patch] = select_index
                    index_score_list[patch] = indexes[n, patch, select_index]
                else:
                    select_index_list[patch] = -1
                    index_score_list[patch] = 1.

            noise = torch.rand_like(index_score_list, device=index_score_list.device)
            patch_sorted = torch.argsort(index_score_list + noise, descending=True) # 按置信度降序排序
            patch_keep = patch_sorted[:n_pred] # 保留置信度高的patch的预测结果
            for patch in patch_keep:
                if mask[n, patch]:
                    index = select_index_list[patch]
                    new_index_map[n, patch] = index
                    # print(indexes[n, patch, index])
                    new_mask[n, patch] = False # 该位置产生预测结果，不再mask
        
        return new_index_map, new_mask


            



